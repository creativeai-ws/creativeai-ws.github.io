<!DOCTYPE html>
<!-- adapted from http://bayesiandeeplearning.org/ -->
<html class="mel_workshop"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>Creative AI Across Modalities | AAAI 2023</title>
		<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
		<meta http-equiv="Pragma" content="no-cache">
		<meta http-equiv="Expires" content="0">
		<meta name="description" content="Embodied Multimodal Learning Workshop | ICLR 2021">
		<meta name="keywords" content="CreativeAI,Multimodal,Learning,Workshop,AAAI,2023">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" href="./CAIAM_files/main.css">
		<meta property="og:title" content="Creative AI Across Modalities | AAAI 2023">
		<meta property="og:type" content="website">
		<meta property="og:url" content="http://creativeai-ws.org">
		<meta property="og:description" content="Creative AI Across Modalities Workshop at AAAI 2023">
	</head>
	<body data-gr-c-s-loaded="true" class="">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<h1>Creative AI Across Modalities</h1>
						<h2><b>AAAI 2023 Workshop (Hybrid)</b></h2>
						<h2>Date: Feb. 13th, 2023</h2>
						<!--<h2><a href="https://iclr.cc/virtual/2021/workshop/2134" target="_blank"><font color="red">Link to ICLR Workshop Virtual Site (Join Zoom)</font></a></h2>-->
					</header>

				<!-- Nav -->
					<nav id="nav" class="">
						<ul>
							<li><a href="https://creativeai-ws.github.io/#abstract" class="active">Abstract</a></li>
							<li><a href="https://creativeai-ws.github.io/#speakers" class="">Invited Speakers</a></li>
							<li><a href="https://creativeai-ws.github.io/#accepted" class="">Accepted Papers</a></li>
							<li><a href="https://creativeai-ws.github.io/#schedule" class="">Schedule</a></li>
							<li><a href="https://creativeai-ws.github.io/#cfp" class="">Call for Papers</a></li>
							<li><a href="https://creativeai-ws.github.io/#organizers" class="">Organizers</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<!-- Introduction -->
							<section id="abstract" class="main">
								<div>
									<div class="content">
										<header class="major">
											<center>
												<h2>Abstract</h2>
											</center>
										</header>
										<h3> 
										For the past few years, we have witnessed eye-opening generation results from AI foundation models such as GPT-3, and DALL-E2. These models have set up great infrastructures for new types of creative generation across various modalities such as language (e.g. story generation), images (e.g. text-to-image generation, fashion design), and audio (e.g. lyrics-to-music generation). Researchers in these fields encounter many similar challenges such as how to use AI to help professional creators, how to evaluate creativity for an AI system, how to boost the creativity of AI, how to avoid negative social impact, and so on. There have been various workshops that focus on some aspects of AI generation. This workshop aims to bridge researchers and practitioners from NLP, computer vision, music, ML, and other computational fields to create the 1st workshop on “Creative AI across Modalities”.
										</h3>
									

									</div>
									<!-- <span class="image"></span> -->
								</div>
							</section>



							<section id="speakers" class="main">
								<div>
									<div class="content">
										<header class="major">
											<center>
												<h2>Invited Speakers</h2>
											</center>
										</header>
										<center>
										<div class="row uniform" align="center">
					 						<div class="3u 12u$(small)">
												<a href="https://sites.google.com/site/snigdhac/" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/snigdha.jpeg" alt="">
													</span>
													<h2>Snigdha Chaturvedi<br> (UNC) </h2>
												</a>
											</div>
											<div class="3u 12u$(small)">
											<a href="https://chrisdonahue.com/" target="_blank" class="image">
												<span class="image fit">
													<img src="./CAIAM_files/chris.png" alt="">
												</span>
												<h2>Chris Donahue<br> (Google Magenta) </h2>
											</a>
											</div>
					 						<div class="3u 12u$(small)">
												<a href="https://andrewowens.com/" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/andrew.jpg" alt="">
													</span>
													<h2>Andrew Owens<br> (UMich) </h2>
												</a>
											</div>
										</div>	
										<div class="row uniform" align="center">
					 						<div class="3u 12u$(small)">
												<a href="https://kittur.org/" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/niki.png" alt="">
													</span>
													<h2>Niki Kittur<br> (CMU) </h2>
												</a>
											</div>
											<div class="3u 12u$(small)">
												<a href="http://eilab.gatech.edu/mark-riedl.html" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/mark_2.png" alt="">
													</span>
													<h2>Mark Riedl<br> (Georgia Tech) </h2>
												</a>
											</div>
					 						<div class="3u 12u$(small)">
												<a href="https://cs.stanford.edu/~diyiy/" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/diyi.jpeg" alt="">
													</span>
													<h2>Diyi Yang<br> (Stanford) </h2>
												</a>
											</div>
					 						<div class="3u 12u$(small)">
												<a href="https://www.dgp.toronto.edu/~hertzman/" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/aarong.png" alt="">
													</span>
													<h2>Aaron Hertzman<br> (Adobe Research) </h2>
												</a>
											</div>
									</div>											
								</div></center>
							</section>


							

							<section id='accepted' class='main'>
								<header class="major">
											<center>
											<h2>Accepted Papers (Camera Readies Coming Soon)</h2>
											</center>
										</header>
								<ul>
									<li><a href="https://openreview.net/forum?id=UQY0bqcl_mX">Photong: Generating 16-Bar Melodies from Images</a> By: Yanjia Zhang, Haohan Wang</li>
<li><a href="https://openreview.net/forum?id=haiht1U7pGL">Large Language Models Learn to Drum</a> By: Li Zhang, Chris Callison-Burch</li>
<li><a href="https://openreview.net/forum?id=Nx9ajnqG9Rw">Blind Judgement: Agent-Based Supreme Court Modelling with GPT</a> By: Sil Hamilton</li>
<li><a href="https://openreview.net/forum?id=UqvWNBQKf5">A Friendly Face: Do Text-to-Image Systems Rely on Stereotypes when the Input is Under-Specified?</a> By: Kathleen C. Fraser, Isar Nejadgholi, Svetlana Kiritchenko</li>
<li><a href="https://openreview.net/forum?id=M24Cs12Gq_A">Spiking ConvLSTM for Semantic Music Generation</a> By: Anna Shvets</li>
<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/8_help_me_write_a_poem_instructi.pdf">Help me write a poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing</a> By: Tuhin Chakrabarty, Vishakh Padmakumar, He He</li>
<li><a href="https://openreview.net/forum?id=QmWXskBhesn">Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Tune Generation Task</a> By: Shangda Wu, Maosong Sun</li>
<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/10_towards_grounded_dialogue_gene.pdf">Towards Grounded Dialogue Generation in Video Game Environments</a> By: Nader Akoury, Ronan Salz, Mohit Iyyer</li>
<li><a href="https://openreview.net/forum?id=8HwKaJ1wvl">Improving the Creativity of Generative Language Models</a> By: Douglas Summers-Stay, Clare R. Voss, Stephanie M. Lukin</li>
<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/13_is_ai_art_another_industrial_r.pdf">Is AI Art Another Industrial Revolution in the Making?</a> By: Alexis Newton, Kaustubh Dhole</li>
<li><a href="https://openreview.net/forum?id=nmtmjfJQLS">Music Playlist Title Generation Using Artist Information</a> By: Haven Kim, Seungheon Doh, Junwon Lee, Juhan Nam</li>
<li><a href="https://openreview.net/forum?id=AtGBXiOAGY">3DStyleMerge: Part-Compatible 3D Style Transfer</a> By: Abhinav Upadhyay, Alpana Dubey, Suma Mani Kuriakose</li>
<li><a href="https://openreview.net/forum?id=tqCf3xklDG">Color Me Intrigued: Quantifying Usage of Colors in Fiction</a> By: Siyan Li</li>
<li><a href="https://openreview.net/forum?id=wm0WZPnhTC">Trash to Treasure: Using text-to-image models to inform the design of physical artefacts</a> By: Hope Schroeder, Amy Smith, Ziv Epstein, Mike Cook, Simon Colton, Andrew Lippman</li>
<li><a href="">Unsupervised Melody-Guided Lyrics Generation</a></li>
<li><a href="https://openreview.net/forum?id=vuqI2p_ZQT">Exploiting Multiple Guidance From 3DMM For Face Reenactment</a> By: Huayu Zhang, Yurui Ren, Yuanqi Chen, Ge Li, Thomas H. Li</li>
<li><a href="https://openreview.net/forum?id=cLBEKlu5WZK">Neural Story Planning</a> By: Anbang Ye, Christopher Zhang Cui, Taiwei Shi, Mark Riedl</li>
<li><a href="https://openreview.net/forum?id=9lmAR2NjTt">Leveraging Human Preferences to Master Poetry</a> By: Rafael Pardinas, Gabriel Huang, David Vazquez, Alexandre Piché</li>
<li><a href="https://openreview.net/forum?id=_8Ity3P03Z1">SEE&TELL: Controllable Narrative Generation from Images</a> By: Stephanie M. Lukin, Sungmin Eum</li>
<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/26_learning_the_visualness_of_tex.pdf">Learning the Visualness of Text Using Large Vision-Language Models</a> By: Gaurav Verma, Ryan A. Rossi, Christopher Tensmeyer, Jiuxiang Gu, Ani Nenkova</li>
<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/27_sketchbetween_video_to_video_s.pdf">SketchBetween: Video-to-Video Synthesis for Sprite Animation via Sketches</a> By: Dagmar Lukka Loftsdóttir, Matthew Guzdial</li>
<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/28_threshold_designer_adaptation_.pdf">Threshold Designer Adaptation: Improved Adaptation for Designers in Co-creative Systems</a> By: Emily Halina, Matthew Guzdial</li>
<li><a href="https://openreview.net/forum?id=UMxeP-FuwyC">Simple Unsupervised Image Captioning via CLIP’s Multimodal Embeddings</a> By: Derek Tam, Colin Raffel, Mohit Bansal</li>
<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/31_culturally_aware_stable_diffus.pdf">Culturally-Aware Stable Diffusion: Supporting Cultural Representation in Text-to-Image Synthesis</a> By: Zhixuan Liu, Peter Schaldenbrand, Youeun Shin, Beverley-Claire Okogwu, Youngsik Yun, Jihie Kim, Jean Oh</li>
<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/33_robot_synesthesia_a_sound_and_.pdf">Robot Synesthesia: A Sound and Semantics Guided AI Painter</a> By: Vihaan Misra, Peter Schaldenbrand, Jean Oh</li>
<li><a href="https://openreview.net/forum?id=phc0KisUnS">Deep Generative Multimedia Children's Literature</a> By: Matthew Lyle Olson</li>
<li><a href="https://github.com/creativeai-ws/creativeai-ws.github.io/blob/main/non-archival_papers/36_diffusion_models_as_visual_rea.pdf">Diffusion Models as Visual Reasoners</a> By: Jason Lin, Maya Srikanth</li>
<li><a href="https://openreview.net/forum?id=3hk5PFxQSG">In BLOOM: Evaluating Creativity and Affinity in Artificial Lyrics and Art</a> By: Evan Crothers, Herna L. Viktor, Nathalie Japkowicz</li>
<li><a href="https://openreview.net/forum?id=FPDRONopLH">Conveying the Predicted Future to Users: A Case Study of Story Plot Prediction</a> By: Chieh-Yang Huang, Saniya Naphade, Kavya Laalasa Karanam, Ting-Hao Huang</li>
<li><a href="https://openreview.net/forum?id=c7W3iTr693">A Tool for Composing Music via Graphic Scores in the style of Gyorgy Ligeti's Artikulation using Self-supervised Representation Learning</a> By: Berker Banar, Simon Colton</li>
<li><a href="https://openreview.net/forum?id=w7n0i0Y4xy">One Artist’s Personal Reflections on Methods and Ethics of Creating Mixed Media Artificial Intelligence Art</a> By: Jane Adams</li>	
								</ul>
								
							</section>





							<section id="schedule" class="main">
								<div>
									<div class="content">
										<header class="major">
											<center>
											<h2>Schedule</h2>
                                                                                        For the in-person participants, please go to Room 146B for joining all the talks and the poster session.
											</center>
										</header>

										<div class="table-wrapper">
											<table class="alt">
												<tbody>
                                                    <col width="15%">
                                                    <col width="15%">
                                                    <col width="15%">
													<tr>
														<td>08:50 am - 09:00 am (EST)</td>
														<td>Introduction and Opening Remarks</td>
														<td></td>
														<td></td>
													</tr>
													<tr>
														<td>09:00 am - 09:45 am (EST)</td>
														<td>Invited Talk</td>
														<td>Andrew Owens<br><font size="2">(UMich)</font></td> 
														<td>Title: Cross-modal Synthesis with Sight, Sound, and Touch</td>
													</tr>
													<tr>
														<td>09:45 am - 10:30 am (EST)</td>
														<td>Invited Talk</td>
														<td>Mark Riedl<br><font size="2">(Georgia Tech)</font></td> 
														<td>Title: Computers, Creativity, and Lovelace <br> <font size="2">Abstract: In this talk we examine the what attributes we should expect in human-level creative systems, and the mechanisms by which we might achieve them. I provide examples from the domain of automated story generation. I conclude the talk with some informal analysis of recent progress toward AI systems that express creativity. </font></td>
													</tr>
													<tr>
														<td>10:30 am - 10:40 am (EST)</td>
														<td>Break</td> 
														<td></td>
														<td></td>
													</tr>
													<tr>
														<td>10:40 am - 11:25 am (EST) </td>
														<td>Invited Talk</td>
														<td>Chris Donahue<br><font size="2">(Google)</font></td> 
														<td>Title: Frontiers in Controllable Music Generation <br> <font size="2">Abstract: For music generation and creative generation more broadly, control is key to unlocking human expression. In this talk, I will discuss the recent improvements in and remaining obstacles to building controllable music generation systems that unlock exciting new expressive capabilities for musicians and non-musicians alike. Additionally, I will discuss control considerations that are more specific to music and argue that text is useful but not sufficient for expressive musical control. As a case study, I will discuss SingSong, a recent system from the MusicLM project at Google which learns to translate vocal performances into instrumental accompaniments, thereby allowing anyone to create rich music featuring their own voice.</font></td>
													</tr>
													<tr>
														<td>11:25 am - 12:10 pm (EST) </td>
														<td>Invited Talk</td>
														<td>Niki Kittur<br><font size="2">(CMU)</font></td> 
														<td>Title: Scaling Analogical Innovation</td>
													</tr>
													<tr>
														<td>12:10 pm - 01:30 pm (EST) </td>
														<td>Lunch</td>
														<td></td>
														<td></td>
													</tr>
													<tr>
														<td>01:30 pm - 02:50 pm (EST) </td>
														<td><b>Poster Session (virtual + in person)</b></td>
														<td></td>
														<td></td>
													</tr>
													<tr>
														<td>02:50 pm - 3:35 pm (EST)</td>
														<td>Invited Talk</td>
														<td>Aaron Hertzman<br><font size="2">(Adobe)</font></td> 
														<td>Title: Can Computers Create Art? <br> <font size="2">Abstract: Can AI algorithms make art, and be considered artists? Within the past decade, the growth of new neural network algorithms has enabled exciting new artforms with considerable public interest, including DeepDream, GANs, VAEs, and diffusion models like DALL-E and Imagen. These tools raise recurring questions about their status as creators and their effect on the arts. In this talk, I will discuss how these developments parallel the development of previous artistic technologies, like oil paint, photography, and traditional computer graphics. I argue that art is a social phenomenon, and discuss possible—but very unlikely—scenarios for when these algorithms could someday be considered artists.</font></td>
													</tr>
													<tr>
														<td>03:35 pm - 04:20 pm (EST)</td>
														<td>Invited Talk</td>
														<td>Snigdha Chaturvedi<br><font size="2">(UNC)</font></td> 
														<td>Title: Modeling People in Automatic Story Generation <br> <font size="2">Abstract: Automatic story generation is the task of designing NLP systems that, given a prompt, can produce the text of a story. Most methods for this problem focus on modeling events and their coherence. However, an alternate perspective to story generation can be from the viewpoint of people described in the story. In this talk, I focus on one aspect of modeling people in story generation -- modeling their social relationships. I describe our story generation approach to incorporate a desired social network demonstrating relationships between various people to be mentioned in the story. We propose a model that uses latent variables to incorporate social relationships. Apart from generating coherent stories that reflect the desired social network, the latent variable-based design results in an explainable generation process. </font></td>
													</tr>
													<tr>
														<td>04:20 pm - 04:30 pm (EST)</td>
														<td>Break</td>
														<td></td>
														<td></td>
													</tr>
													<tr>
														<td>04:30 pm - 05:15 pm (EST)</td>
														<td>Invited Talk</td>
														<td>Diyi Yang<br><font size="2">(Stanford)</font></td> 
														<td>Title: Improving Everyday Interaction through Human-Centered Text Generation  <br> <font size="2">Abstract: As natural language generation has gained popularity and produced extensive industrial applications, there has been an increasing focus on enabling the use of natural language in human-like interactions. How can we improve such everyday interactions and build language generation systems that are more aware of human factors?  In this talk,  we take a closer look at human-centric language generation and present two recent works that promote positive language use and summarize daily conversations.  Specifically, the first part examines positive reframing by neutralizing a negative point of view and generating a more positive perspective without contradicting the original meaning.  The second part demonstrates how more structures of conversations can be utilized to generate better summaries for everyday conversation.</font></td>
													</tr>
													<tr>
														<td>05:15 pm - 05:25 pm (EST)</td>
														<td>Closing Remarks</td>
														<td></td> 
														<td></td>
													</tr>
												</tbody>
											</table>
										</div>
										
											<!--<h2>Accepted Papers</h2>
										</center>
	<table>
												<tbody>
<col width="40%">
<col width="60%">
<tr><td><b>
Title
</b></td><td><b>
Authors
</b></td><td><b>
Paper Session
</b></td></tr>
<tr><td><b><a href="./Papers/A1.pdf">ABC Problem: An Investigation of Offline RL for Vision-Based Dynamic Manipulation</a></b></td><td>Kamyar Ghassemipour, Igor Mordatch, Shixiang Shane Gu</td><td><b>A1</b></td></tr>
<tr><td><b><a href="./Papers/A2.pdf">Language Acquisition is Embodied, Interactive, Emotive: a Research Proposal</a></b></td><td>Casey Kennington</td><td><b>A2</b></td></tr>
<tr><td><b><a href="./Papers/A3.pdf">Ask & Explore: Grounded Question Answering for Curiosity-Driven Exploration</a></b></td><td>Jivat Neet Kaur, Yiding Jiang, Paul Pu Liang</td><td><b>A3</b></td></tr>
<tr><td><b><a href="./Papers/A4.pdf">Towards Teaching Machines with Language: Interactive Learning From Only Language Descriptions of Activities</a></b></td><td>Khanh Nguyen, Dipendra Misra, Robert Schapire, Miroslav Dudik, Patrick Shafto</td><td><b>A4</b></td></tr>
<tr><td><b><a href="./Papers/A5.pdf">YouRefIt: Embodied Reference Understanding with Language and Gesture</b></td><td>Yixin Chen, Qing Li, Deqian Kong, Yik Lun Kei, Tao Gao, Yixin Zhu, Song-Chun Zhu, Siyuan Huang</td><td></a><b>A5</b></td></tr>
<tr><td><b><a href="./Papers/B1.pdf">Learning to Set Waypoints for Audio-Visual Navigation</b></td><td>Changan Chen, Sagnik Majumder, Ziad Al-Halah, Ruohan Gao, Santhosh K. Ramakrishnan, Kristen Grauman</td><td></a><b>B1</b></td></tr>
<tr><td><b><a href="./Papers/B2.pdf">Semantic Audio-Visual Navigation</b></td><td>Changan Chen, Ziad Al-Halah, Kristen Grauman</a></td><td><b>B2</b></td></tr>
<tr><td><b>Attentive Feature Reuse for Multi Task Meta learning</b></td><td>Kiran Lekkala, Laurent Itti</a></td><td><b>B3</b></td></tr>
<tr><td><b><a href="./Papers/B4.pdf">SeLaVi: self-labelling videos without any annotations from scratch</b></td><td>Yuki Asano, Mandela Patric, Christian Rupprecht, Andrea Vedaldi</td><td><b>B4</b></td></tr>

												</tbody>
											</table>
									</div>
								</div>
							</section>-->
<section id="cfp" class="main">
								<div>
									<div class="content">
										<header class="major">
											<center>
											<h2>Call for Papers</h2>
											</center>
										</header>
										<h3> Authors are invited to send the following relevant work, either archival or non-archival, in the <a href="https://www.aaai.org/Publications/Templates/AuthorKit23.zip" target="_blank">AAAI-23 proceedings format</a>:
										<li> Long paper: Submission of original work up to seven pages for contents and one page for references.</li>
										<li> Short paper: Submission of work in progress with preliminary results, and position papers, up to four pages for contents and one page for references.</li>
										Topics including but not limited to:	
										<ul>
												<li>Creative language generation: stories, poetry, figurative languages. </li>
												<li>Generative model and algorithms for image/audio, and multi-modal/video generation.</li>
												<li>Theory and analysis for creativity (e.g., humor understanding)</li>
												<li>Detecting and quantifying creativity</li>
												<li> Using AI to improve human creativity (e.g., HCI+ML studies to accelerate scientific novelty)</li>
												<li>Data and resources for creative generation</li>
												<li>Applications of creative AI generation, such as automatic video dubbing</li>
												<li>Novel evaluation for creative AI generated outputs</li>
												<li>Social, cultural, and ethical considerations of creative AI generations, such as racial/gender bias, trustworthiness</li>
										</ul>
											A submission should take the form of a AAAI long or short paper in PDF format using the <a href="https://www.aaai.org/Publications/Templates/AuthorKit23.zip" target="_blank">AAAI style</a>. We will accept submissions of (1) papers that have not been previously published or accepted for publication in substantially similar form; (2) papers that have been published or accepted for publication in recent venues including journal, conference, workshop, and arXiv; and (3) research proposals for future work with a focus on well-defined concepts and ideas. All submissions will be reviewed with double blind policy.
										</h3>
										<br>
										
										<h2>Open Review submissions website: <a href="https://openreview.net/group?id=AAAI.org/2023/Workshop/creativeAI">https://openreview.net/group?id=AAAI.org/2023/Workshop/creativeAI</a><h2>

										<br>
										<h2>Key Dates:
											<h3>
											<ul>
												<li>Submission deadline: Nov. 18, 2022 (11:59 p.m. Anywhere on Earth)</li>
												<li>Notification to authors: Dec. 20, 2022 (11:59 p.m. Anywhere on Earth)</li>
												<li>Workshop date: Feb. 13, 2023</li>
											</ul>
											</h3>
										</h2>


										<!--<h2>Program Committee:
											<h3>
												Unnat Jain (UIUC), Michelle Lee (Stanford), Paul Pu Liang (CMU), Senthil Purushwalkam (CMU), Santhosh Kumar Ramakrishnan (UT Austin), Mohit Shridhar (UW), Tianmin Shu (MIT), Shaoxiong Wang (MIT)
											</h3>
										</h2>-->


									</div>
								</div>
							</section>

							<section id="organizers" class="main special">
								<div>
									<div class="content" align="center">
										<header class="major">
											<h2>Organizers</h2>
										</header>

										<div class="row uniform" align="center">
											<div class="1u 0u$(small)">
												<a href="#" class="image"></a>
											</div>
											<div class="2u 12u$(small)">
												<a href="https://sites.google.com/view/drjinghuang" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/jing.jpg" alt="">
													</span>
													<h3>Jing Huang <br> (Amazon)</h2>
												</a>
											</div>
											<div class="2u 12u$(small)">
												<a href="http://prithvirajva.com/" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/raj_1.png" alt="">
													</span>
													<h3>Prithviraj (Raj) Ammanabrolu<br> (AI2)</h2>
												</a>
											</div>
											<div class="2u 12u$(small)">
												<a href="https://vnpeng.net/" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/violet.png" alt="">
													</span>
													<h3>Violet Peng<br> (UCLA)</h2>
												</a>
											</div>
											<div class="2u 12u$(small)">
												<a href="https://www.cs.unc.edu/~mbansal/" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/mohit.png" alt="">
													</span>
													<h3>Mohit Bansal<br> (UNC)</h2>
												</a>
											</div>
											<div class="2u 12u$(small)">
												<a href="https://jiajunwu.com/" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/jiajun.jpeg" alt="">
													</span>
													<h3>Jiajun Wu<br> (Stanford)</h2>
												</a>
											</div>
										</div>
										<div class="row uniform" align="center">
											<div class="1u 0u$(small)">
												<a href="#" class="image"></a>
											</div>
											<div class="2u 12u$(small)">
												<a href="https://www.linkedin.com/in/arindam-mandal" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/arindam.jpeg" alt="">
													</span>
													<h3>Arindam Mandal<br> (Amazon)</h2>
												</a>
											</div>
											<div class="2u 12u$(small)">
												<a href="https://ken77921.github.io/" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/hawshiuan.webp" alt="">
													</span>
													<h3>Haw-Shiuan Chang<br> (Amazon)</h2>
												</a>
											</div>
											<div class="2u 12u$(small)">
												<a href="https://ai.stanford.edu/~rhgao/" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/ruohan.jpg" alt="">
													</span>
													<h3>Ruohan Gao <br> (Stanford)</h2>
												</a>
											</div>
											<div class="2u 12u$(small)">
												<a href="https://users.soe.ucsc.edu/~hannahbrahman/" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/faeze.png" alt="">
													</span>
													<h3>Faeze Brahman <br> (AI2)</h2>
												</a>
											</div>
											<div class="2u 12u$(small)">
												<a href="https://cseweb.ucsd.edu/~jmcauley/" target="_blank" class="image">
													<span class="image fit">
														<img src="./CAIAM_files/julian.jpeg" alt="">
													</span>
													<h3>Julian McAuley <br> (UCSD)</h2>
												</a>
											</div>
											
										</div>
									</div>
								</div>
							</section>


					</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">Website design adapted from <a href="http://bayesiandeeplearning.org/">Yarin Gal </a> and based on <a href="https://html5up.net/">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script async="" src="./MEL_files/analytics.js"></script><script src="./CAIAM_files/jquery.min.js"></script>
			<script src="./CAIAM_files/jquery.scrollex.min.js"></script>
			<script src="./CAIAM_files/jquery.scrolly.min.js"></script>
			<script src="./CAIAM_files/skel.min.js"></script>
			<script src="./CAIAM_files/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="./CAIAM_files/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JC1QVS8GW2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-JC1QVS8GW2');
</script>
	
</body></html>
